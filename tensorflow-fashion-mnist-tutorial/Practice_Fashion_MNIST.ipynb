{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice Fashion-MNIST",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "S2Xw8ywSZdf9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A little bit about Google Colaboratory"
      ]
    },
    {
      "metadata": {
        "id": "nVkV53yUZIVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8yiPZsSUZM5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fYulL-7vY8xq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VqXYHwe4ZESc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tR4hiqFyeXb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup\n"
      ]
    },
    {
      "metadata": {
        "id": "D_Z_uzq9C1Gb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kmnnyIjGGW3x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zalandoresearch/fashion-mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCyI4HGzEwp8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQyTEnbDZjUx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## We will visualize the training with Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "0WBohMzC-XDl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TB_DIR = './Graph'\n",
        "\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(TB_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8cY1PJheT30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading and understanding the dataset"
      ]
    },
    {
      "metadata": {
        "id": "A8PABKspHdVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz --directory-prefix=data/fashion\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz --directory-prefix=data/fashion\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz --directory-prefix=data/fashion\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz --directory-prefix=data/fashion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wALr0mmF-yx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sys.path.insert(0, 'fashion-mnist/utils')\n",
        "import mnist_reader\n",
        "\n",
        "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
        "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-GE4IoK36keS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qRpp7RQeXiAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_new = np.zeros((len(X_train), 28, 28))\n",
        "for i in range(len((X_train))):\n",
        "    X_new[i] = X_train[i].reshape([28,28])\n",
        "X_train = X_new\n",
        "\n",
        "X_new = np.zeros((len(X_test), 28, 28))\n",
        "for i in range(len((X_test))):\n",
        "    X_new[i] = X_test[i].reshape([28,28])\n",
        "X_test = X_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aSNJHVZVX7wQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXPum1_uNLGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Image data"
      ]
    },
    {
      "metadata": {
        "id": "gz1vr5EZJlja",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.max(X_train[0]), np.min(X_train[0]), np.mean(X_train), np.std(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y11XXamvJCgV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Labels\n"
      ]
    },
    {
      "metadata": {
        "id": "TWKubTlvJJzA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each training and test example is assigned to one of the following labels:\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |"
      ]
    },
    {
      "metadata": {
        "id": "y_lbgTL5I0Y7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_lbl = {\n",
        "    0 : 'T-shirt/top',\n",
        "    1 : 'Trouser',\n",
        "    2 : 'Pullover',\n",
        "    3 : 'Dress',\n",
        "    4 : 'Coat',\n",
        "    5 : 'Sandal',\n",
        "    6 : 'Shirt',\n",
        "    7 : 'Sneaker',\n",
        "    8 : 'Bag',\n",
        "    9 : 'Angle Boot'\n",
        "}\n",
        "\n",
        "print(set(y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9J7Av2gnNIJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing images"
      ]
    },
    {
      "metadata": {
        "id": "3AfB_217eTPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "\n",
        "for i in range(1, 5):\n",
        "    idx = random.randint(0, len(X_train))\n",
        "    plt.subplot(220+i)\n",
        "    plt.imshow(255-X_train[idx].reshape((28,28,)),\n",
        "               cmap=plt.get_cmap('gray'), origin='upper')\n",
        "    plt.setp(plt.title(int_to_lbl[y_train[idx]]), color='b') \n",
        "\n",
        "    plt.grid(None)\n",
        "    plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPEzsBIixITM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code"
      ]
    },
    {
      "metadata": {
        "id": "4rQl6ORjv1lG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, 'fashion-mnist/utils')\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import mnist_reader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugnLqp9axHwu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TB_DIR = './Graph'\n",
        "tf.set_random_seed(0)\n",
        "\n",
        "def get_data():\n",
        "    X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
        "    X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
        "        \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  \n",
        "def make_input_fn(X, y, batch_size, mode):\n",
        "    def preprocess(image, label):\n",
        "        image = (image-72.9403)/90.0211\n",
        "        image = tf.reshape(image, shape=(28,28,1))\n",
        "        return image, label\n",
        "      \n",
        "    def _input_fn():\n",
        "        \n",
        "        ds = tf.data.Dataset.from_tensor_slices((\n",
        "            tf.cast(X, tf.float32),\n",
        "            tf.cast(y, tf.int32)))\n",
        "                    \n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            ds = ds.apply(tf.data.experimental.shuffle_and_repeat(\n",
        "                buffer_size=3*batch_size, count=None))\n",
        "        else:\n",
        "            ds = ds.repeat(1)\n",
        "            \n",
        "        ds = ds.apply(tf.data.experimental.map_and_batch(\n",
        "             map_func=preprocess, batch_size=batch_size))\n",
        "        ds = ds.prefetch(buffer_size=5000)\n",
        "                      \n",
        "        return ds\n",
        "    return _input_fn\n",
        "\n",
        "\n",
        "def train_and_evaluate(model_dir, hparams, model_fn_for_train):\n",
        "    \n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn = model_fn_for_train,\n",
        "        params = hparams,\n",
        "        config= tf.estimator.RunConfig(\n",
        "            save_checkpoints_steps = 2000,\n",
        "            log_step_count_steps=1000\n",
        "            ),\n",
        "        model_dir = model_dir)\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = get_data()\n",
        "\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = make_input_fn(\n",
        "            X_train, y_train,\n",
        "            hparams['batch_size'],\n",
        "            mode = tf.estimator.ModeKeys.TRAIN),\n",
        "        max_steps = (50000//hparams[\"batch_size\"]) * hparams[\"epochs\"])\n",
        "    \n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = make_input_fn(\n",
        "            X_test, y_test,\n",
        "            hparams['batch_size'],\n",
        "            mode = tf.estimator.ModeKeys.EVAL\n",
        "        ),\n",
        "        start_delay_secs = 1,\n",
        "        throttle_secs = 1\n",
        "    )\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r_YNyOYBOgyL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.0 fully connected single layer"
      ]
    },
    {
      "metadata": {
        "id": "0uMn4S3_xPz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_1.0'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features)\n",
        "    W = tf.Variable(tf.zeros([784, 10]))\n",
        "    b = tf.Variable(tf.zeros([10]))\n",
        "    \n",
        "    XX = tf.reshape(features, [-1, 784])\n",
        "\n",
        "    # The model\n",
        "    Y = tf.nn.softmax(tf.matmul(XX, W) + b)\n",
        "    \n",
        "    tf.summary.histogram(W.name.replace(':', '_'), W)\n",
        "    tf.summary.histogram(b.name.replace(':', '_'), b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # loss function\n",
        "        cross_entropy = -tf.reduce_sum(tf.one_hot(labels, 10) * tf.log(Y))\n",
        "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            tf.summary.scalar(\"learning_rate\", params[\"learning_rate\"])\n",
        "            optimizer = tf.train.GradientDescentOptimizer(params[\"learning_rate\"])\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "    \"batch_size\": 100,\n",
        "    \"learning_rate\": 0.000003,\n",
        "    \"pkeep\": 0.70,\n",
        "    \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6nhvUUwiOjGB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.0 fully connected going deep + stable cross entropy"
      ]
    },
    {
      "metadata": {
        "id": "sz907OtX0eg5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params[\"learning_rate\"] = 0.003"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1xIjwZd4h3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_2.0'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features)\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 200\n",
        "    M = 100\n",
        "    N = 60\n",
        "    O = 30\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([784, L])),  # 784 = 28 * 28\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([L, M])),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([M, N])),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([N, O])),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10]))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.zeros([L])),\n",
        "        \"B2\" : tf.Variable(tf.zeros([M])),\n",
        "        \"B3\" : tf.Variable(tf.zeros([N])),\n",
        "        \"B4\" : tf.Variable(tf.zeros([O])),\n",
        "        \"B5\" : tf.Variable(tf.zeros([10]))\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    XX = tf.reshape(features, [-1, 784])\n",
        "    Y1 = tf.nn.sigmoid(tf.matmul(XX, weights[\"W1\"]) + biases[\"B1\"])\n",
        "    Y2 = tf.nn.sigmoid(tf.matmul(Y1, weights[\"W2\"]) + biases[\"B2\"])\n",
        "    Y3 = tf.nn.sigmoid(tf.matmul(Y2, weights[\"W3\"]) + biases[\"B3\"])\n",
        "    Y4 = tf.nn.sigmoid(tf.matmul(Y3, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Ylogits = tf.matmul(Y4, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "      \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "\n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            tf.summary.scalar(\"learning_rate\", params[\"learning_rate\"])\n",
        "            optimizer = tf.train.GradientDescentOptimizer(params[\"learning_rate\"])\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  params = {\n",
        "    \"batch_size\": 200,\n",
        "    \"learning_rate\": 0.000003,\n",
        "    \"pkeep\": 0.75,\n",
        "    \"epochs\": 50\n",
        "}\n",
        "\n",
        "params[\"learning_rate\"] = 0.003\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "apsV7iBNjL8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 initialization + learning rate decay + relu"
      ]
    },
    {
      "metadata": {
        "id": "ERRECa2WjVus",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_2.1'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features)\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 200\n",
        "    M = 100\n",
        "    N = 60\n",
        "    O = 30\n",
        "    # Weights initialised with small random values between -0.2 and +0.2\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([784, L], stddev=0.1)),  # 784 = 28 * 28\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    XX = tf.reshape(features, [-1, 784])\n",
        "    Y1 = tf.nn.relu(tf.matmul(XX, weights[\"W1\"]) + biases[\"B1\"])\n",
        "    Y2 = tf.nn.relu(tf.matmul(Y1, weights[\"W2\"]) + biases[\"B2\"])\n",
        "    Y3 = tf.nn.relu(tf.matmul(Y2, weights[\"W3\"]) + biases[\"B3\"])\n",
        "    Y4 = tf.nn.relu(tf.matmul(Y3, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Ylogits = tf.matmul(Y4, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "    \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xIWZ5hJCzKLb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 dropout"
      ]
    },
    {
      "metadata": {
        "id": "zGYixVCHzK09",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_2.2'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features)\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 200\n",
        "    M = 100\n",
        "    N = 60\n",
        "    O = 30\n",
        "    # Weights initialised with small random values between -0.2 and +0.2\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([784, L], stddev=0.1)),  # 784 = 28 * 28\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    XX = tf.reshape(features, [-1, 784])\n",
        "    Y1 = tf.nn.relu(tf.matmul(XX, weights[\"W1\"]) + biases[\"B1\"])\n",
        "    Y1d = tf.nn.dropout(Y1, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y2 = tf.nn.relu(tf.matmul(Y1d, weights[\"W2\"]) + biases[\"B2\"])\n",
        "    Y2d = tf.nn.dropout(Y2, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y3 = tf.nn.relu(tf.matmul(Y2d, weights[\"W3\"]) + biases[\"B3\"])\n",
        "    Y3d = tf.nn.dropout(Y3, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y4 = tf.nn.relu(tf.matmul(Y3d, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Y4d = tf.nn.dropout(Y4, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "    Ylogits = tf.matmul(Y4d, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "    \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jA52VIbJzYyl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.0 convolutional + AdamOptimizer\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wJvXSp_8zZHq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_3.0'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features)\n",
        "\n",
        "    # three convolutional layers with their channel counts, and a\n",
        "    # fully connected layer (tha last layer has 10 softmax neurons)\n",
        "    L = 4  # first convolutional layer output depth\n",
        "    M = 8  # second convolutional layer output depth\n",
        "    N = 12  # third convolutional layer\n",
        "    O = 200  # fully connected layer\n",
        "\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([5, 5, 1, L], stddev=0.1)),  # 5x5 patch, 1 input channel, K output channels\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([5, 5, L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([4, 4, M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([7 * 7 * N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    stride = 1  # output is 28x28\n",
        "    Y1 = tf.nn.relu(tf.nn.conv2d(features, weights[\"W1\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B1\"])\n",
        "    stride = 2  # output is 14x14\n",
        "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, weights[\"W2\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B2\"])\n",
        "    stride = 2  # output is 7x7\n",
        "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, weights[\"W3\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B3\"])\n",
        "\n",
        "    # reshape the output from the third convolution for the fully connected layer\n",
        "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * N])\n",
        "\n",
        "    Y4 = tf.nn.relu(tf.matmul(YY, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Ylogits = tf.matmul(Y4, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "    \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4wbs8nfe1Z9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1 more neurons"
      ]
    },
    {
      "metadata": {
        "id": "20ZBHO6Z1aUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_3.1'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features)\n",
        "\n",
        "    # three convolutional layers with their channel counts, and a\n",
        "    # fully connected layer (tha last layer has 10 softmax neurons)\n",
        "    L = 6  # first convolutional layer output depth\n",
        "    M = 12  # second convolutional layer output depth\n",
        "    N = 24  # third convolutional layer\n",
        "    O = 200  # fully connected layer\n",
        "\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([6, 6, 1, L], stddev=0.1)),  # 5x5 patch, 1 input channel, K output channels\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([5, 5, L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([4, 4, M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([7 * 7 * N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    stride = 1  # output is 28x28\n",
        "    Y1 = tf.nn.relu(tf.nn.conv2d(features, weights[\"W1\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B1\"])\n",
        "    stride = 2  # output is 14x14\n",
        "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, weights[\"W2\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B2\"])\n",
        "    stride = 2  # output is 7x7\n",
        "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, weights[\"W3\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B3\"])\n",
        "\n",
        "    # reshape the output from the third convolution for the fully connected layer\n",
        "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * N])\n",
        "\n",
        "    Y4 = tf.nn.relu(tf.matmul(YY, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Y4d = tf.nn.dropout(Y4, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "    Ylogits = tf.matmul(Y4d, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "    \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ShpDx5kMJMCk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.0 Convolutional + batch normalization + leaky relu"
      ]
    },
    {
      "metadata": {
        "id": "QdR4uCh_JNkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_4.0'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features)\n",
        "\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 24\n",
        "    M = 48\n",
        "    N = 64\n",
        "    O = 200\n",
        "\n",
        "    # Weights initialised with small random values between -0.2 and +0.2\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([6, 6, 1, L], stddev=0.1)),  # 5x5 patch, 1 input channel, K output channels\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([5, 5, L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([4, 4, M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([7 * 7 * N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    biases = {\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }  \n",
        "\n",
        "    def compatible_convolutional_noise_shape(Y):\n",
        "        noiseshape = tf.shape(Y)\n",
        "        noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n",
        "        return noiseshape\n",
        "\n",
        "    # The model\n",
        "    stride = 1  # output is 28x28\n",
        "    Y1l = tf.nn.conv2d(features, weights[\"W1\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y1bn = tf.layers.batch_normalization(Y1l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y1r = tf.nn.leaky_relu(Y1bn)\n",
        "    Y1 = tf.nn.dropout(Y1r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y1r))\n",
        "    stride = 2  # output is 14x14\n",
        "    Y2l = tf.nn.conv2d(Y1, weights[\"W2\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y2bn = tf.layers.batch_normalization(Y2l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y2r = tf.nn.leaky_relu(Y2bn)\n",
        "    Y2 = tf.nn.dropout(Y2r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y2r))\n",
        "    stride = 2  # output is 7x7\n",
        "    Y3l = tf.nn.conv2d(Y2, weights[\"W3\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y3bn = tf.layers.batch_normalization(Y3l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y2r = tf.nn.leaky_relu(Y3bn)\n",
        "    Y3 = tf.nn.dropout(Y2r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y2r))\n",
        "    \n",
        "    # reshape the output from the third convolution for the fully connected layer\n",
        "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * N])\n",
        "\n",
        "    Y4l = tf.matmul(YY, weights[\"W4\"]) + biases[\"B4\"]\n",
        "    Y4bn = tf.layers.batch_normalization(Y4l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y4r = tf.nn.relu(Y4bn)\n",
        "    Y4 = tf.nn.dropout(Y4r, params[\"pkeep\"])\n",
        "    Ylogits = tf.matmul(Y4, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "            # this is needed for batch normalization, but has no effect otherwise\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "            with tf.control_dependencies(update_ops):\n",
        "                train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxUCZzgEdI54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# clean all"
      ]
    },
    {
      "metadata": {
        "id": "XnlMYilEO2L0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r Graph/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1jgnupiga0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls Graph/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q54jSgO60PAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}