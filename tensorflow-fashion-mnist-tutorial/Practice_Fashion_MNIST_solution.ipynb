{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice Fashion-MNIST",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "S2Xw8ywSZdf9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A little bit about Google Colaboratory"
      ]
    },
    {
      "metadata": {
        "id": "nVkV53yUZIVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8yiPZsSUZM5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fYulL-7vY8xq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VqXYHwe4ZESc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tR4hiqFyeXb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup\n"
      ]
    },
    {
      "metadata": {
        "id": "JlF4hQ01Gs8x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gitpython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g3aP1dlPF8Zb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unzip_file(file_name, local_path):\n",
        "  from zipfile import ZipFile\n",
        "  \n",
        "  zip_ref = ZipFile(file_name, 'r')\n",
        "  zip_ref.extractall(local_path)\n",
        "  zip_ref.close()\n",
        "\n",
        "  \n",
        "def maybe_download_files(file_names, base_url, local_path):\n",
        "  from urllib.request import urlretrieve, urljoin\n",
        "  from tqdm import tqdm_notebook as tqdm\n",
        "  from os.path import join, isfile, exists\n",
        "  from os import makedirs\n",
        "  \n",
        "  if not exists(local_path):\n",
        "    makedirs(local_path)\n",
        "    \n",
        "  for file_name in tqdm(file_names):\n",
        "    url       = urljoin(url_base, file_name)\n",
        "    file_path = join(local_path, file_name)\n",
        "    if not isfile(file_path):\n",
        "      urlretrieve(url, file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvqM1rKa0FrF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url_base  = \"https://bin.equinox.io/c/4VmDzA7iaHb/\" \n",
        "file_names = [\"ngrok-stable-linux-amd64.zip\"]\n",
        "\n",
        "maybe_download_files(file_names, url_base, '.')\n",
        "\n",
        "unzip_file(file_names[0], '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bdJtPOD6eFLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm ngrok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKaLUq880gCE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_repo_name(git_url):\n",
        "  from urllib.parse import urlsplit\n",
        "  from os.path import splitext, basename\n",
        "  \n",
        "  return splitext(basename(urlsplit(git_url).path))[0]\n",
        "  \n",
        "  \n",
        "def maybe_clone(local_path, git_url):\n",
        "  from os.path import exists\n",
        "  from git import Git\n",
        "  \n",
        "  git_name = get_repo_name(git_url)\n",
        "  if not exists(git_name):\n",
        "    Git(fashion_mnist_path).clone(fashion_mnist_repo_url)\n",
        "    print(fashion_mnist_repo_url, \"cloned to\", git_name)\n",
        "  else:\n",
        "    print(fashion_mnist_repo_url, \"already local at\", git_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kmnnyIjGGW3x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fashion_mnist_path = \".\"\n",
        "fashion_mnist_repo_url = \"https://github.com/zalandoresearch/fashion-mnist\"    \n",
        "    \n",
        "maybe_clone(fashion_mnist_path, fashion_mnist_repo_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQyTEnbDZjUx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## We will visualize the training with Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "0WBohMzC-XDl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TB_DIR = './Graph'\n",
        "\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(TB_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8cY1PJheT30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading and understanding the dataset"
      ]
    },
    {
      "metadata": {
        "id": "5TngMlNRCFE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url_base = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
        "DATA_PATH = \"data/fashion\" \n",
        "file_names = [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\", \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]\n",
        "\n",
        "\n",
        "maybe_download_files(file_names, url_base, DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wALr0mmF-yx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'fashion-mnist/utils')\n",
        "import mnist_reader\n",
        "\n",
        "X_train, y_train = mnist_reader.load_mnist(DATA_PATH, kind='train')\n",
        "X_test, y_test = mnist_reader.load_mnist(DATA_PATH, kind='t10k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-GE4IoK36keS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qRpp7RQeXiAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_new = np.zeros((len(X_train), 28, 28))\n",
        "for i in range(len((X_train))):\n",
        "    X_new[i] = X_train[i].reshape([28,28])\n",
        "X_train = X_new\n",
        "\n",
        "X_new = np.zeros((len(X_test), 28, 28))\n",
        "for i in range(len((X_test))):\n",
        "    X_new[i] = X_test[i].reshape([28,28])\n",
        "X_test = X_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aSNJHVZVX7wQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXPum1_uNLGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Image data"
      ]
    },
    {
      "metadata": {
        "id": "gz1vr5EZJlja",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.max(X_train[0]), np.min(X_train[0]), np.mean(X_train), np.std(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y11XXamvJCgV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Labels\n"
      ]
    },
    {
      "metadata": {
        "id": "TWKubTlvJJzA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each training and test example is assigned to one of the following labels:\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |"
      ]
    },
    {
      "metadata": {
        "id": "y_lbgTL5I0Y7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "int_to_lbl = {\n",
        "    0 : 'T-shirt/top',\n",
        "    1 : 'Trouser',\n",
        "    2 : 'Pullover',\n",
        "    3 : 'Dress',\n",
        "    4 : 'Coat',\n",
        "    5 : 'Sandal',\n",
        "    6 : 'Shirt',\n",
        "    7 : 'Sneaker',\n",
        "    8 : 'Bag',\n",
        "    9 : 'Angle Boot'\n",
        "}\n",
        "\n",
        "{int_to_lbl[k]: v for k, v in collections.Counter(y_train).items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9J7Av2gnNIJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing images"
      ]
    },
    {
      "metadata": {
        "id": "3AfB_217eTPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "\n",
        "for i in range(1, 5):\n",
        "    idx = random.randint(0, len(X_train))\n",
        "    plt.subplot(220+i)\n",
        "    plt.imshow(255-X_train[idx].reshape((28,28,)),\n",
        "               cmap=plt.get_cmap('gray'), origin='upper')\n",
        "    plt.setp(plt.title(int_to_lbl[y_train[idx]]), color='b') \n",
        "\n",
        "    plt.grid(None)\n",
        "    plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPEzsBIixITM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Support Code"
      ]
    },
    {
      "metadata": {
        "id": "4rQl6ORjv1lG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    \n",
        "maybe_clone(\"https://github.com/zalandoresearch/fashion-mnist\" , \".\")\n",
        "\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, 'fashion-mnist/utils')\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import mnist_reader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugnLqp9axHwu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TB_DIR = './Graph'\n",
        "tf.set_random_seed(0)\n",
        "\n",
        "def get_data():\n",
        "    X_train, y_train = mnist_reader.load_mnist(DATA_PATH, kind='train')\n",
        "    X_test, y_test = mnist_reader.load_mnist(DATA_PATH, kind='t10k')\n",
        "        \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  \n",
        "def make_input_fn(X, y, batch_size, mode):\n",
        "    def preprocess(image, label):\n",
        "        image = (image-72.9403)/90.0211\n",
        "        image = tf.reshape(image, shape=(28,28,1))\n",
        "        return {'image_data' : image}, label\n",
        "      \n",
        "    def _input_fn():\n",
        "        \n",
        "        ds = tf.data.Dataset.from_tensor_slices((\n",
        "            tf.cast(X, tf.float32),\n",
        "            tf.cast(y, tf.int32)))\n",
        "                    \n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            ds = ds.apply(tf.data.experimental.shuffle_and_repeat(\n",
        "                buffer_size=3*batch_size, count=None))\n",
        "        else:\n",
        "            ds = ds.repeat(1)\n",
        "            \n",
        "        ds = ds.apply(tf.data.experimental.map_and_batch(\n",
        "             map_func=preprocess, batch_size=batch_size))\n",
        "        ds = ds.prefetch(buffer_size=5000)\n",
        "                      \n",
        "        return ds\n",
        "    return _input_fn\n",
        "\n",
        "\n",
        "def train_and_evaluate(model_dir, hparams, model_fn_for_train):\n",
        "    \n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn = model_fn_for_train,\n",
        "        params = hparams,\n",
        "        config= tf.estimator.RunConfig(\n",
        "            save_checkpoints_steps = 2000,\n",
        "            log_step_count_steps=1000\n",
        "            ),\n",
        "        model_dir = model_dir)\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = get_data()\n",
        "\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = make_input_fn(\n",
        "            X_train, y_train,\n",
        "            hparams['batch_size'],\n",
        "            mode = tf.estimator.ModeKeys.TRAIN),\n",
        "        max_steps = (50000//hparams[\"batch_size\"]) * hparams[\"epochs\"])\n",
        "    \n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = make_input_fn(\n",
        "            X_test, y_test,\n",
        "            hparams['batch_size'],\n",
        "            mode = tf.estimator.ModeKeys.EVAL\n",
        "        ),\n",
        "        start_delay_secs = 1,\n",
        "        throttle_secs = 1\n",
        "    )\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gjqBTnTl_5k8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fully Connected Models"
      ]
    },
    {
      "metadata": {
        "id": "r_YNyOYBOgyL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.0 fully connected single layer"
      ]
    },
    {
      "metadata": {
        "id": "0uMn4S3_xPz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_1.0'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "    W = tf.Variable(tf.zeros([784, 10]))\n",
        "    b = tf.Variable(tf.zeros([10]))\n",
        "    \n",
        "    XX = tf.reshape(features[\"image_data\"], [-1, 784])\n",
        "\n",
        "    # The model\n",
        "    Y = tf.nn.softmax(tf.matmul(XX, W) + b)\n",
        "    \n",
        "    tf.summary.histogram(W.name.replace(':', '_'), W)\n",
        "    tf.summary.histogram(b.name.replace(':', '_'), b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # loss function\n",
        "        cross_entropy = -tf.reduce_sum(tf.one_hot(labels, 10) * tf.log(Y))\n",
        "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            tf.summary.scalar(\"learning_rate\", params[\"learning_rate\"])\n",
        "            optimizer = tf.train.GradientDescentOptimizer(params[\"learning_rate\"])\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "    \"batch_size\": 100,\n",
        "    \"learning_rate\": 0.000003,\n",
        "    \"pkeep\": 0.75,\n",
        "    \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6nhvUUwiOjGB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.0 fully connected going deep + stable cross entropy"
      ]
    },
    {
      "metadata": {
        "id": "x1xIjwZd4h3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_2.0'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 200\n",
        "    M = 100\n",
        "    N = 60\n",
        "    O = 30\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([784, L])),  # 784 = 28 * 28\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([L, M])),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([M, N])),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([N, O])),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10]))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.zeros([L])),\n",
        "        \"B2\" : tf.Variable(tf.zeros([M])),\n",
        "        \"B3\" : tf.Variable(tf.zeros([N])),\n",
        "        \"B4\" : tf.Variable(tf.zeros([O])),\n",
        "        \"B5\" : tf.Variable(tf.zeros([10]))\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    XX = tf.reshape(features[\"image_data\"], [-1, 784])\n",
        "    Y1 = tf.nn.sigmoid(tf.matmul(XX, weights[\"W1\"]) + biases[\"B1\"])\n",
        "    Y2 = tf.nn.sigmoid(tf.matmul(Y1, weights[\"W2\"]) + biases[\"B2\"])\n",
        "    Y3 = tf.nn.sigmoid(tf.matmul(Y2, weights[\"W3\"]) + biases[\"B3\"])\n",
        "    Y4 = tf.nn.sigmoid(tf.matmul(Y3, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Ylogits = tf.matmul(Y4, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "      \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "\n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            tf.summary.scalar(\"learning_rate\", params[\"learning_rate\"])\n",
        "            optimizer = tf.train.GradientDescentOptimizer(params[\"learning_rate\"])\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.70,\n",
        "  \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "apsV7iBNjL8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 initialization + learning rate decay + relu"
      ]
    },
    {
      "metadata": {
        "id": "ERRECa2WjVus",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_2.1'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 200\n",
        "    M = 100\n",
        "    N = 60\n",
        "    O = 30\n",
        "    # Weights initialised with small random values between -0.2 and +0.2\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([784, L], stddev=0.1)),  # 784 = 28 * 28\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    XX = tf.reshape(features[\"image_data\"], [-1, 784])\n",
        "    Y1 = tf.nn.relu(tf.matmul(XX, weights[\"W1\"]) + biases[\"B1\"])\n",
        "    Y2 = tf.nn.relu(tf.matmul(Y1, weights[\"W2\"]) + biases[\"B2\"])\n",
        "    Y3 = tf.nn.relu(tf.matmul(Y2, weights[\"W3\"]) + biases[\"B3\"])\n",
        "    Y4 = tf.nn.relu(tf.matmul(Y3, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Ylogits = tf.matmul(Y4, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "    \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.70,\n",
        "  \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xIWZ5hJCzKLb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 dropout"
      ]
    },
    {
      "metadata": {
        "id": "zGYixVCHzK09",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_2.2'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 200\n",
        "    M = 100\n",
        "    N = 60\n",
        "    O = 30\n",
        "    # Weights initialised with small random values between -0.2 and +0.2\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([784, L], stddev=0.1)),  # 784 = 28 * 28\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    XX = tf.reshape(features[\"image_data\"], [-1, 784])\n",
        "    Y1 = tf.nn.relu(tf.matmul(XX, weights[\"W1\"]) + biases[\"B1\"])\n",
        "    Y1d = tf.nn.dropout(Y1, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y2 = tf.nn.relu(tf.matmul(Y1d, weights[\"W2\"]) + biases[\"B2\"])\n",
        "    Y2d = tf.nn.dropout(Y2, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y3 = tf.nn.relu(tf.matmul(Y2d, weights[\"W3\"]) + biases[\"B3\"])\n",
        "    Y3d = tf.nn.dropout(Y3, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y4 = tf.nn.relu(tf.matmul(Y3d, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Y4d = tf.nn.dropout(Y4, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "    Ylogits = tf.matmul(Y4d, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "    \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.75,\n",
        "  \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJmJgabqAC0X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Homework"
      ]
    },
    {
      "metadata": {
        "id": "96xRkeXDvEHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below you have an example with the `fashion_1.0` model implemented using the **Sequential** modeling from `tensorflow.keras`:"
      ]
    },
    {
      "metadata": {
        "id": "n6FovGK1OoQg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers # Documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "\n",
        "from tensorflow.keras.models import Sequential # https://keras.io/getting-started/sequential-model-guide/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VYHVIuf1IW7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_1.0 easy_way'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "\n",
        "    # The model\n",
        "    model = Sequential([\n",
        "        layers.Dense(10, input_shape=(784,), activation='softmax', use_bias=True, kernel_initializer='zeros', bias_initializer='zeros'),\n",
        "    ])\n",
        "    \n",
        "    XX = tf.reshape(features[\"image_data\"], [-1, 784])\n",
        "    Y = model(XX)\n",
        "    \n",
        "    W, b = model.layers[0].weights\n",
        "    tf.summary.histogram(W.name, W)\n",
        "    tf.summary.histogram(b.name, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # loss function\n",
        "        cross_entropy = -tf.reduce_sum(tf.one_hot(labels, 10) * tf.log(Y))\n",
        "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            tf.summary.scalar(\"learning_rate\", params[\"learning_rate\"])\n",
        "            optimizer = tf.train.GradientDescentOptimizer(params[\"learning_rate\"])\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "    \"batch_size\": 100,\n",
        "    \"learning_rate\": 0.000003,\n",
        "    \"pkeep\": 0.75,\n",
        "    \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RhWSrnRCUfub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below are some useful links are below:\n",
        "\n",
        "1. https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "2. https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential\n",
        "3. https://www.tensorflow.org/api_docs/python/tf/keras/initializers/TruncatedNormal\n",
        "4.  https://keras.io/getting-started/sequential-model-guide/\n",
        "\n",
        "\n",
        "### Exercise 1: replace the \"hard way\" code from `fashion_2.2` with the \"easy way\" using `tensorlfow.keras` higher interface:"
      ]
    },
    {
      "metadata": {
        "id": "pFtC5zsRb9-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential and https://keras.io/getting-started/sequential-model-guide/\n",
        "from tensorflow.keras.models import Sequential \n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/initializers/TruncatedNormal\n",
        "from tensorflow.keras.initializers import TruncatedNormal as init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rkL2mr2AAEd6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_2.2 easy_way'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    \"\"\"\n",
        "    L = 200\n",
        "    M = 100\n",
        "    N = 60\n",
        "    O = 30\n",
        "    # Weights initialised with small random values between -0.2 and +0.2\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([784, L], stddev=0.1)),  # 784 = 28 * 28\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    Y1 = tf.nn.relu(tf.matmul(XX, weights[\"W1\"]) + biases[\"B1\"])\n",
        "    Y1d = tf.nn.dropout(Y1, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y2 = tf.nn.relu(tf.matmul(Y1d, weights[\"W2\"]) + biases[\"B2\"])\n",
        "    Y2d = tf.nn.dropout(Y2, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y3 = tf.nn.relu(tf.matmul(Y2d, weights[\"W3\"]) + biases[\"B3\"])\n",
        "    Y3d = tf.nn.dropout(Y3, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "\n",
        "    Y4 = tf.nn.relu(tf.matmul(Y3d, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Y4d = tf.nn.dropout(Y4, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "    Ylogits = tf.matmul(Y4d, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\"\"\"\n",
        "\n",
        "    # The model\n",
        "    model = Sequential([\n",
        "        layers.Dense(200, activation='relu',\n",
        "                     kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                     bias_initializer=init(mean=0.0, stddev=0.1)),\n",
        "        layers.Dropout(params[\"pkeep\"]),\n",
        "\n",
        "        layers.Dense(100, activation='relu',\n",
        "                     kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                     bias_initializer=init(mean=0.0, stddev=0.1)),\n",
        "        layers.Dropout(params[\"pkeep\"]),\n",
        "\n",
        "        layers.Dense(60, activation='relu',\n",
        "                     kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                     bias_initializer=init(mean=0.0, stddev=0.1)),\n",
        "        layers.Dropout(params[\"pkeep\"]),\n",
        "\n",
        "        layers.Dense(30, activation='relu',\n",
        "                     kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                     bias_initializer=init(mean=0.0, stddev=0.1)),\n",
        "        layers.Dropout(params[\"pkeep\"]),\n",
        "\n",
        "        layers.Dense(10, kernel_initializer=init(), bias_initializer=init())\n",
        "    ])\n",
        "    \n",
        "    XX = tf.reshape(features[\"image_data\"], [-1, 784])\n",
        "    \n",
        "    Ylogits = model(XX)\n",
        "    Y = layers.Activation('softmax')(Ylogits)\n",
        "    \n",
        "    # Warning: the next 4 lines might not work depending on how you create your model,\n",
        "    # if you receive an error from these lines you can comment the next 4 lines or take it as a challeng to understand why ;)\n",
        "    for layer in model.layers:\n",
        "        if layer.weights:\n",
        "            W, b = layer.weights\n",
        "            tf.summary.histogram(W.name, W)\n",
        "            tf.summary.histogram(b.name, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.75,\n",
        "  \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UTjxpAeS_-I0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Models"
      ]
    },
    {
      "metadata": {
        "id": "jA52VIbJzYyl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.0 convolutional + AdamOptimizer\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wJvXSp_8zZHq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_3.0'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "\n",
        "    # three convolutional layers with their channel counts, and a\n",
        "    # fully connected layer (tha last layer has 10 softmax neurons)\n",
        "    L = 4  # first convolutional layer output depth\n",
        "    M = 8  # second convolutional layer output depth\n",
        "    N = 12  # third convolutional layer\n",
        "    O = 200  # fully connected layer\n",
        "\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([5, 5, 1, L], stddev=0.1)),  # 5x5 patch, 1 input channel, K output channels\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([5, 5, L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([4, 4, M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([7 * 7 * N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    stride = 1  # output is 28x28\n",
        "    Y1 = tf.nn.relu(tf.nn.conv2d(features[\"image_data\"], weights[\"W1\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B1\"])\n",
        "    stride = 2  # output is 14x14\n",
        "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, weights[\"W2\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B2\"])\n",
        "    stride = 2  # output is 7x7\n",
        "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, weights[\"W3\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B3\"])\n",
        "\n",
        "    # reshape the output from the third convolution for the fully connected layer\n",
        "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * N])\n",
        "\n",
        "    Y4 = tf.nn.relu(tf.matmul(YY, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Y4d = tf.nn.dropout(Y4, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "    Ylogits = tf.matmul(Y4d, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "    \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.75,\n",
        "  \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4wbs8nfe1Z9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1 more neurons"
      ]
    },
    {
      "metadata": {
        "id": "20ZBHO6Z1aUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_3.1'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "\n",
        "    # three convolutional layers with their channel counts, and a\n",
        "    # fully connected layer (tha last layer has 10 softmax neurons)\n",
        "    L = 6  # first convolutional layer output depth\n",
        "    M = 12  # second convolutional layer output depth\n",
        "    N = 24  # third convolutional layer\n",
        "    O = 200  # fully connected layer\n",
        "\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([6, 6, 1, L], stddev=0.1)),  # 5x5 patch, 1 input channel, K output channels\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([5, 5, L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([4, 4, M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([7 * 7 * N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    biases = {\n",
        "        \"B1\" : tf.Variable(tf.ones([L])/10),\n",
        "        \"B2\" : tf.Variable(tf.ones([M])/10),\n",
        "        \"B3\" : tf.Variable(tf.ones([N])/10),\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }   \n",
        "\n",
        "    # The model\n",
        "    stride = 1  # output is 28x28\n",
        "    Y1 = tf.nn.relu(tf.nn.conv2d(features[\"image_data\"], weights[\"W1\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B1\"])\n",
        "    stride = 2  # output is 14x14\n",
        "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, weights[\"W2\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B2\"])\n",
        "    stride = 2  # output is 7x7\n",
        "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, weights[\"W3\"], strides=[1, stride, stride, 1], padding='SAME') + biases[\"B3\"])\n",
        "\n",
        "    # reshape the output from the third convolution for the fully connected layer\n",
        "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * N])\n",
        "\n",
        "    Y4 = tf.nn.relu(tf.matmul(YY, weights[\"W4\"]) + biases[\"B4\"])\n",
        "    Y4d = tf.nn.dropout(Y4, params[\"pkeep\"] if mode == tf.estimator.ModeKeys.TRAIN else 1.0)\n",
        "    Ylogits = tf.matmul(Y4d, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "    \n",
        "    for k, w in weights.items():\n",
        "        tf.summary.histogram(k, w)\n",
        "    for k, b in biases.items():\n",
        "        tf.summary.histogram(k, b)\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "            train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "\n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.75,\n",
        "  \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ShpDx5kMJMCk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.0 Convolutional + batch normalization + leaky relu"
      ]
    },
    {
      "metadata": {
        "id": "QdR4uCh_JNkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_name = 'fashion_4.0'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "\n",
        "    # five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 12\n",
        "    M = 46\n",
        "    N = 64\n",
        "    O = 200\n",
        "\n",
        "    # Weights initialised with small random values between -0.2 and +0.2\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([6, 6, 1, L], stddev=0.1)),  # 5x5 patch, 1 input channel, K output channels\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([5, 5, L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([4, 4, M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([7 * 7 * N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    biases = {\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }  \n",
        "\n",
        "    def compatible_convolutional_noise_shape(Y):\n",
        "        noiseshape = tf.shape(Y)\n",
        "        noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n",
        "        return noiseshape\n",
        "\n",
        "    # The model\n",
        "    stride = 1  # output is 28x28\n",
        "    Y1l = tf.nn.conv2d(features[\"image_data\"], weights[\"W1\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y1bn = tf.layers.batch_normalization(Y1l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y1r = tf.nn.leaky_relu(Y1bn)\n",
        "    Y1 = tf.nn.dropout(Y1r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y1r))\n",
        "    stride = 2  # output is 14x14\n",
        "    Y2l = tf.nn.conv2d(Y1, weights[\"W2\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y2bn = tf.layers.batch_normalization(Y2l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y2r = tf.nn.leaky_relu(Y2bn)\n",
        "    Y2 = tf.nn.dropout(Y2r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y2r))\n",
        "    stride = 2  # output is 7x7\n",
        "    Y3l = tf.nn.conv2d(Y2, weights[\"W3\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y3bn = tf.layers.batch_normalization(Y3l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y2r = tf.nn.leaky_relu(Y3bn)\n",
        "    Y3 = tf.nn.dropout(Y2r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y2r))\n",
        "    \n",
        "    # reshape the output from the third convolution for the fully connected layer\n",
        "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * N])\n",
        "\n",
        "    Y4l = tf.matmul(YY, weights[\"W4\"]) + biases[\"B4\"]\n",
        "    Y4bn = tf.layers.batch_normalization(Y4l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y4r = tf.nn.relu(Y4bn)\n",
        "    Y4 = tf.nn.dropout(Y4r, params[\"pkeep\"])\n",
        "    Ylogits = tf.matmul(Y4, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\n",
        "\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "            # this is needed for batch normalization, but has no effect otherwise\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "            with tf.control_dependencies(update_ops):\n",
        "                train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.75,\n",
        "  \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_h4dJCrYEZt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Samples to keep studying"
      ]
    },
    {
      "metadata": {
        "id": "VGtyJO4IYghn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.  replace the \"hard way\" code with the \"easy way\" using keras higher interface for a convolutional architecture:"
      ]
    },
    {
      "metadata": {
        "id": "idh03Xb4YHqb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential and https://keras.io/getting-started/sequential-model-guide/\n",
        "from tensorflow.keras.models import Sequential \n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/initializers/TruncatedNormal\n",
        "from tensorflow.keras.initializers import TruncatedNormal as init\n",
        "\n",
        "\n",
        "model_name = 'fashion_4.0 easy_way'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "\n",
        "    \"\"\"# five layers and their number of neurons (tha last layer has 10 softmax neurons)\n",
        "    L = 24\n",
        "    M = 48\n",
        "    N = 64\n",
        "    O = 200\n",
        "\n",
        "    # Weights initialised with small random values between -0.2 and +0.2\n",
        "    weights = {\n",
        "        \"W1\" : tf.Variable(tf.truncated_normal([6, 6, 1, L], stddev=0.1)),  # 5x5 patch, 1 input channel, K output channels\n",
        "        \"W2\" : tf.Variable(tf.truncated_normal([5, 5, L, M], stddev=0.1)),\n",
        "        \"W3\" : tf.Variable(tf.truncated_normal([4, 4, M, N], stddev=0.1)),\n",
        "        \"W4\" : tf.Variable(tf.truncated_normal([7 * 7 * N, O], stddev=0.1)),\n",
        "        \"W5\" : tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
        "    }\n",
        "    # When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\n",
        "    biases = {\n",
        "        \"B4\" : tf.Variable(tf.ones([O])/10),\n",
        "        \"B5\" : tf.Variable(tf.ones([10])/10)\n",
        "    }  \n",
        "\n",
        "    def compatible_convolutional_noise_shape(Y):\n",
        "        noiseshape = tf.shape(Y)\n",
        "        noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n",
        "        return noiseshape\n",
        "\n",
        "    # The model\n",
        "    stride = 1  # output is 28x28\n",
        "    Y1l = tf.nn.conv2d(features, weights[\"W1\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y1bn = tf.layers.batch_normalization(Y1l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y1r = tf.nn.leaky_relu(Y1bn)\n",
        "    Y1 = tf.nn.dropout(Y1r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y1r))\n",
        "    stride = 2  # output is 14x14\n",
        "    Y2l = tf.nn.conv2d(Y1, weights[\"W2\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y2bn = tf.layers.batch_normalization(Y2l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y2r = tf.nn.leaky_relu(Y2bn)\n",
        "    Y2 = tf.nn.dropout(Y2r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y2r))\n",
        "    stride = 2  # output is 7x7\n",
        "    Y3l = tf.nn.conv2d(Y2, weights[\"W3\"], strides=[1, stride, stride, 1], padding='SAME')\n",
        "    Y3bn = tf.layers.batch_normalization(Y3l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y2r = tf.nn.leaky_relu(Y3bn)\n",
        "    Y3 = tf.nn.dropout(Y2r, params[\"pkeep\"], compatible_convolutional_noise_shape(Y2r))\n",
        "    \n",
        "    # reshape the output from the third convolution for the fully connected layer\n",
        "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * N])\n",
        "\n",
        "    Y4l = tf.matmul(YY, weights[\"W4\"]) + biases[\"B4\"]\n",
        "    Y4bn = tf.layers.batch_normalization(Y4l, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    Y4r = tf.nn.relu(Y4bn)\n",
        "    Y4 = tf.nn.dropout(Y4r, params[\"pkeep\"])\n",
        "    Ylogits = tf.matmul(Y4, weights[\"W5\"]) + biases[\"B5\"]\n",
        "    Y = tf.nn.softmax(Ylogits)\"\"\"\n",
        "    \n",
        "    #TODO: replace the commented \"hard way\" code above with the \"easier way\" using keras higher interface\n",
        "    conv_blocks_starters = [\n",
        "        layers.Conv2D(filters=24, kernel_size=6, strides=1, padding='same',\n",
        "                      kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                      bias_initializer=init(mean=0.0, stddev=0.1)),\n",
        "        \n",
        "        layers.Conv2D(filters=48, kernel_size=5, strides=2, padding='same',\n",
        "                  kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                  bias_initializer=init(mean=0.0, stddev=0.1)),\n",
        "        \n",
        "        layers.Conv2D(filters=64, kernel_size=4, strides=2, padding='same',\n",
        "                      kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                      bias_initializer=init(mean=0.0, stddev=0.1)),\n",
        "    ]\n",
        "    \n",
        "    fc_block_starters = [\n",
        "        layers.Dense(200, use_bias=True,\n",
        "                     kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                     bias_initializer=init(mean=0.0, stddev=0.1))\n",
        "    ]\n",
        "        \n",
        "    model = Sequential()\n",
        "    \n",
        "    def add_block(model, layer):\n",
        "        model.add(layer)\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.LeakyReLU())\n",
        "        model.add(layers.Dropout(params[\"pkeep\"]))\n",
        "        return model\n",
        "      \n",
        "    for layer in conv_blocks_starters:\n",
        "        model = add_block(model, layer)\n",
        "        \n",
        "    #model.add(layers.Reshape((-1, 7 * 7 * 64)))\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    for layer in fc_block_starters:\n",
        "        model = add_block(model, layer)\n",
        "    \n",
        "    model.add(layers.Dense(10, kernel_initializer=init(), bias_initializer=init()))\n",
        "    \n",
        "    Ylogits = model(features[\"image_data\"])\n",
        "    Y = layers.Activation('softmax')(Ylogits)\n",
        "    \n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "            # this is needed for batch normalization, but has no effect otherwise\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "            with tf.control_dependencies(update_ops):\n",
        "                train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.75,\n",
        "  \"epochs\": 50\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z6NyWT1mYmfU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.  use a \"famous\" architecture\n",
        "\n",
        "Some famous architectures have an input size requirement, normally \"famous\" convolutionary architectures deal with images with more than 32x32 pixels, smallest. That is why I provided you with a new `make_input_fn` function below. This new function is resizing the image to a bigger number of pixels, from 28x28 to 32x32. :)\n",
        "\n",
        "Good luck and have fun!\n"
      ]
    },
    {
      "metadata": {
        "id": "sq1HB2dLmbfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "# train_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in train_X])\n",
        "# test_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in test_X])\n",
        "\n",
        "\n",
        "def make_input_fn(X, y, batch_size, mode):\n",
        "    def preprocess(image, label):\n",
        "        image = (image-72.9403)/90.0211\n",
        "        image = tf.reshape(image, shape=(28,28,1))\n",
        "        image = tf.expand_dims(image, 0) # This is needed for the function below to work correctly\n",
        "        image = tf.image.resize_bilinear(image, size=(32,32)) # Here the image is resized\n",
        "        image = tf.squeeze(image, 0) # This is correcting the change we did applying `expand_dims`\n",
        "        return {'image_data' : image}, label\n",
        "      \n",
        "    def _input_fn():\n",
        "        \n",
        "        ds = tf.data.Dataset.from_tensor_slices((\n",
        "            tf.cast(X, tf.float32),\n",
        "            tf.cast(y, tf.int32)))\n",
        "                    \n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            ds = ds.apply(tf.data.experimental.shuffle_and_repeat(\n",
        "                buffer_size=3*batch_size, count=None))\n",
        "        else:\n",
        "            ds = ds.repeat(1)\n",
        "            \n",
        "        ds = ds.apply(tf.data.experimental.map_and_batch(\n",
        "             map_func=preprocess, batch_size=batch_size))\n",
        "        ds = ds.prefetch(buffer_size=5000)\n",
        "                      \n",
        "        return ds\n",
        "    return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yHKD9qyGbciZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.initializers import TruncatedNormal as init\n",
        "\n",
        "model_name = 'fashion_vgg16'\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    tf.summary.image('image', features[\"image_data\"])\n",
        "    \n",
        "    #TODO: use a famous architecture like VGG16\n",
        "    base_model = VGG16(#weights='imagenet',\n",
        "        weights = None, include_top=False, input_tensor=features[\"image_data\"])\n",
        "    \n",
        "    feature_map = base_model(features[\"image_data\"])\n",
        "    \n",
        "    xx = layers.Flatten()(feature_map)\n",
        "\n",
        "    xx = layers.Dense(200, use_bias=True,\n",
        "                     kernel_initializer=init(mean=0.0, stddev=0.1),\n",
        "                     bias_initializer=init(mean=0.0, stddev=0.1))(xx)\n",
        "    xx = layers.BatchNormalization()(xx)\n",
        "    xx = layers.LeakyReLU()(xx)\n",
        "    xx = layers.Dropout(params[\"pkeep\"])(xx)\n",
        "    \n",
        "    Ylogits = layers.Dense(10, kernel_initializer=init(), bias_initializer=init())(xx)\n",
        "\n",
        "    Y = layers.Activation('softmax')(Ylogits)\n",
        "    \n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
        "        # TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical instability\n",
        "        # problems with log(0) which is NaN\n",
        "        cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "            weights=params[\"batch_size\"],\n",
        "            onehot_labels=tf.one_hot(labels, 10),\n",
        "            logits=Ylogits)\n",
        "        \n",
        "        # % of correct answers found in batch\n",
        "        predictions = tf.argmax(Y,1)\n",
        "        accuracy = tf.metrics.accuracy(predictions, labels)\n",
        "\n",
        "        evalmetrics = {\"accuracy/mnist\": accuracy}\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            tf.summary.scalar(\"accuracy/mnist\", accuracy[1])\n",
        "            # the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/max_steps)), i.e. exponential decay from 0.003->0.0001\n",
        "            max_steps = (50000//params[\"batch_size\"]) * params[\"epochs\"]\n",
        "            lr = 0.0001 +  tf.train.exponential_decay(params[\"learning_rate\"],\n",
        "                tf.train.get_global_step(), max_steps, 1/math.e)\n",
        "            tf.summary.scalar(\"learning_rate\", lr)\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "            # this is needed for batch normalization, but has no effect otherwise\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "            with tf.control_dependencies(update_ops):\n",
        "                train_step = optimizer.minimize(cross_entropy,\n",
        "                                            global_step=tf.train.get_global_step())\n",
        "        else:\n",
        "            train_step = None\n",
        "    else:\n",
        "        cross_entropy = None\n",
        "        train_step = None\n",
        "        evalmetrics = None\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions={\"classid\": predictions},\n",
        "            loss=cross_entropy,\n",
        "            train_op=train_step,\n",
        "            eval_metric_ops=evalmetrics)\n",
        "\n",
        "  \n",
        "params = {\n",
        "  \"batch_size\": 100,\n",
        "  \"learning_rate\": 0.003,\n",
        "  \"pkeep\": 0.75,\n",
        "  \"epochs\": 30\n",
        "}\n",
        "train_and_evaluate(os.path.join(TB_DIR, model_name), params, model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxUCZzgEdI54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# clean all"
      ]
    },
    {
      "metadata": {
        "id": "XnlMYilEO2L0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r Graph/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1jgnupiga0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls Graph/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q54jSgO60PAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}